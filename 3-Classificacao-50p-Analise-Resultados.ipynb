{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Resultados da Previsão de Engajamento - Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 3119,
     "status": "ok",
     "timestamp": 1678973020538,
     "user": {
      "displayName": "PABLO SAMPAIO",
      "userId": "06578877141428254753"
     },
     "user_tz": 180
    },
    "id": "OJV4GlB7-Jqx",
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#@title Importações de pacote\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import load_results, filter_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "BASE_PATH = 'dados/preprocessed/'\n",
    "TYPE_NAME = 'paraphrase-multilingual-MiniLM'\n",
    "SOCIAL_NETWORK = 'tiktok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERE AQUI !!!\n",
    "RESULTS_FILE_PATH = \"resultados/resultados50p_2025-04-02-13h48m_lula(mxbai-embed-large-v1)_(tiktok).npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xg4V0QFFLo7g"
   },
   "source": [
    "# 1 - Carregando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando os dados dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, results = load_results(RESULTS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_SocVpnTp-p"
   },
   "source": [
    "Carregando dados brutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1678973021364,
     "user": {
      "displayName": "PABLO SAMPAIO",
      "userId": "06578877141428254753"
     },
     "user_tz": 180
    },
    "id": "pbGadD5L-aMf",
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dfx_full = pd.read_excel(f\"{BASE_PATH}full-preproc2-inputs_{TYPE_NAME}_{SOCIAL_NETWORK}.xlsx\", index_col='ID')\n",
    "dfy_full = pd.read_excel(f\"{BASE_PATH}full-preproc2-outputs_{TYPE_NAME}_{SOCIAL_NETWORK}.xlsx\", index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx, dfy = filter_dataset(dfx_full, dfy_full, subdataset=metadata['subdataset'], target_col=metadata['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_MODEL_NAMES = metadata['basic_models']\n",
    "BASIC_MODEL_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Métricas Principais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos disponíveis (básicos e ensemble)\n",
    "print(\"Modelos testados:\")\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#METRICS = ['f1_score_mean', 'f1_score_std', 'precision_mean', 'precision_std', 'recall_mean', 'recall_std', 'accuracy_mean', 'accuracy_std']\n",
    "METRICS = ['F1_score_mean', 'F1_score_std', 'Precisão_mean', 'Precisão_std', 'Revocação_mean', 'Revocação_std', 'Acurácia_mean', 'Acurácia_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(results).T\n",
    "df_result = df_result[METRICS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.loc[df_result.index.isin(BASIC_MODEL_NAMES), :].sort_values('F1_score_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def order_models_by(order_by):\n",
    "    display(df_result.sort_values(by=order_by, ascending=False))\n",
    "    return\n",
    "\n",
    "interact(order_models_by, order_by=METRICS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classificador Aleatório**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def random_classifier_results(y_true, chance_for_class1=0.5, trials=100):\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for i in range(trials):\n",
    "        y_pred = np.random.choice([0, 1], size=len(y_true), p=[1.0-chance_for_class1, chance_for_class1])\n",
    "        accuracies.append(metrics.accuracy_score(y_true, y_pred))\n",
    "        precisions.append(metrics.precision_score(y_true, y_pred))\n",
    "        recalls.append(metrics.recall_score(y_true, y_pred))\n",
    "        f1s.append(metrics.f1_score(y_true, y_pred))\n",
    "    print(f\"RANDOM CLASSIFIER ({100*chance_for_class1:.0f}% chance for class 1)\")\n",
    "    print(f\"- Accuracy: {np.mean(accuracies):.4f} +/- {np.std(accuracies):.5f}\") \n",
    "    print(f\"- Precision: {np.mean(precisions):.4f} +/- {np.std(precisions):.5f}\")\n",
    "    print(f\"- Recall: {np.mean(recalls):.4f} +/- {np.std(recalls):.5f}\")\n",
    "    print(f\"- F1-Score: {np.mean(f1s):.4f} +/- {np.std(f1s):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "random_classifier_results(dfy, 0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Melhores Hiper-Parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Valores de parâmetros mais frequentes entre os melhores, por modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def aggregate_best_params(model_name):\n",
    "    best_params_list = results[model_name]['melhores_parametros']\n",
    "    \n",
    "    # for each name of parameter, counts the number of times each of its value appeared among the best\n",
    "    params_count = dict()\n",
    "\n",
    "    for param_name in best_params_list[0].keys():\n",
    "        # fora each value of the parameter, counts the occurences\n",
    "        params_count[param_name] = Counter()\n",
    "        \n",
    "        for params in best_params_list:\n",
    "            if param_name == 'preproc':\n",
    "                param_value = params[param_name]._simplified_name\n",
    "            else:\n",
    "                param_value = params[param_name]\n",
    "            params_count[param_name][param_value] += 1\n",
    "    \n",
    "    display(params_count)\n",
    "\n",
    "#aggregate_best_params('Logistic Regression')\n",
    "interact(aggregate_best_params, model_name=BASIC_MODEL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Melhor modelo por fold (e seus hiperparâmetros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def list_best_model_per_fold():\n",
    "    for fold in range(5):   \n",
    "        best_f1 = 0.0\n",
    "        best_model_params = None\n",
    "        best_model_name = None\n",
    "        for model_name in results.keys():\n",
    "            # ignora modelos ensemble\n",
    "            if 'melhores_parametros' not in results[model_name]:\n",
    "                continue\n",
    "            \n",
    "            if results[model_name]['F1_score_list'][fold] > best_f1:\n",
    "                best_f1 = results[model_name]['F1_score_list'][fold]\n",
    "                best_model_params = results[model_name]['melhores_parametros'][fold]\n",
    "                best_model_name = model_name\n",
    "        \n",
    "        print(f\"FOLD {fold} - best model is a '{best_model_name}' with hiper parameters:\")\n",
    "        for key in best_model_params:\n",
    "            if key == 'preproc':\n",
    "                print(f\" - {key}: {best_model_params[key]._simplified_name}\")\n",
    "            else:\n",
    "                print(f\" - {key}: {best_model_params[key]}\")\n",
    "        print(f\" - F1 score: {results[best_model_name]['F1_score_list'][fold]:.4f}\")\n",
    "\n",
    "\n",
    "list_best_model_per_fold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Melhores hiperparâmetros de cada modelo (em todos os folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs.: A seção 3.1 mostra este resultado de forma mais condensada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def list_all_best_params(model_name):\n",
    "    best_params_list = results[model_name]['melhores_parametros']\n",
    "    for i, paramset in enumerate(best_params_list):\n",
    "        print(\"BEST PARAM SET IN FOLD\", i, \":\")\n",
    "        for key in paramset.keys():\n",
    "            if key == 'preproc':\n",
    "                print(f\" - {key}: {paramset[key]._simplified_name}\")\n",
    "            else:\n",
    "                print(f\" - {key}: {paramset[key]}\")\n",
    "        print(f\" - F1 score: {results[model_name]['F1_score_list'][i]:.4f}\")\n",
    "\n",
    "\n",
    "#list_all_best_params('Logistic Regression')\n",
    "#list_all_best_params('Random Forest')\n",
    "#list_all_best_params('Support Vector Machine')\n",
    "\n",
    "interact(list_all_best_params, model_name=BASIC_MODEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[\"Support Vector Machine\"][\"melhores_modelos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmujMeBpoAkU"
   },
   "source": [
    "# 4 - Importâncias dos Atributos(não sei se a gente vai estudar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UT5hVGjgAAwi"
   },
   "source": [
    "- Baseado nos valores de `feature_importances_` calculados em alguns modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1678973723605,
     "user": {
      "displayName": "PABLO SAMPAIO",
      "userId": "06578877141428254753"
     },
     "user_tz": 180
    },
    "id": "AfddNtwOrJyF",
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#@title Definições auxiliares (não precisa abrir)\n",
    "from sklearn.tree import plot_tree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_importance(importances, title=None, max_features=None, feature_names=None, sort=True, ax=None):\n",
    "    \"\"\"\n",
    "    Plot feature importances for a scikit-learn random forest or gradient boosting model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        A scikit-learn random forest or gradient boosting model.\n",
    "    title: str or None, optional (default=None)\n",
    "        The title of the plot.\n",
    "    max_features : int or None, optional (default=None)\n",
    "        The maximum number of features to plot. If None, plot all features.\n",
    "    feature_names : list or None, optional (default=None)\n",
    "        A list of feature names to use in the plot. If None, use the feature indices.\n",
    "    sort : bool, optional (default=True)\n",
    "        Whether to sort the feature importances in descending order.\n",
    "    ax : matplotlib.axes.Axes or None, optional (default=None)\n",
    "        The matplotlib axes to plot the feature importances on. If None, create a new figure and axes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The matplotlib axes containing the plot.\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract feature importances\n",
    "    #importances = model.feature_importances_\n",
    "\n",
    "    # Get feature names\n",
    "    if feature_names is None:\n",
    "        feature_names = [str(i) for i in range(len(importances))]\n",
    "\n",
    "    # Sort feature importances\n",
    "    if sort:\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        importances = importances[indices]\n",
    "        feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "    # Truncate feature importances\n",
    "    if max_features is not None:\n",
    "        importances = importances[:max_features]\n",
    "        feature_names = feature_names[:max_features]\n",
    "\n",
    "    # Create plot\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "   \n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=12)\n",
    "\n",
    "    ax.barh(np.arange(len(importances)), importances, align='center')\n",
    "    ax.set_yticks(np.arange(len(importances)))\n",
    "    ax.set_yticklabels(feature_names)\n",
    "    #ax.set_xlabel('Gini importance', fontsize=10)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if 'Decision Tree' in results:\n",
    "    # best model of the 1st fold of the outer cross-validation\n",
    "    dec_tree_pipeline = results['Decision Tree']['melhores_modelos'][0]\n",
    "\n",
    "    # fit the pipeline to the WHOLE dataset\n",
    "    dec_tree_pipeline.fit(dfx, dfy)\n",
    "\n",
    "    display(dec_tree_pipeline)\n",
    "    \n",
    "    # the decision tree model inside the pipeline\n",
    "    dec_tree = dec_tree_pipeline.named_steps['predictor']\n",
    "    feature_names = dec_tree_pipeline.named_steps['preproc'].get_feature_names_out()\n",
    "\n",
    "    fig = plt.figure(figsize=(22,15))\n",
    "    plot_tree(dec_tree, feature_names=list(feature_names), filled=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_translate = {'cat__': '', 'remainder__': '', \n",
    "                    'DiasDecorridos': 'Elapsed Days', \n",
    "                    'Dispositivo Retórico': 'Rhetorical Device',\n",
    "                    'Retórica Aristotélica': 'Aristotelian Rhetoric',\n",
    "                    'Tipo de conteúdo': 'Content Type',\n",
    "                    'Texto': 'Text',\n",
    "                    'Tonalidade': 'Tone',\n",
    "                    'Duracao': 'Duration',\n",
    "                    'Abordagem': 'Approach'}\n",
    "\n",
    "def translate_feature_names(feature_names):\n",
    "    translated_names = []\n",
    "    for name in feature_names:\n",
    "        for k in to_translate.keys():\n",
    "            name = name.replace(k, to_translate[k])\n",
    "        translated_names.append(name)\n",
    "    return translated_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostras as importâncias para CADA um dos 5 modelos (dos 5 folds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in results.keys():\n",
    "    if 'vote' in model_name:\n",
    "        continue\n",
    "\n",
    "    model_pipeline = results[model_name]['melhores_modelos'][0]  # the best model found in the first fold\n",
    "    model_predictor = model_pipeline.named_steps['predictor']\n",
    "\n",
    "    # testa se o modelo guarda as importâncias das features\n",
    "    if not hasattr(model_predictor, 'feature_importances_'):\n",
    "        print(f\"O modelo {model_name} não registra as importâncias das features\")\n",
    "        continue\n",
    "\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(20, 50))\n",
    "    \n",
    "    # Configurando a validação cruzada externa (igual à da função de treinamento)\n",
    "    cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=metadata['random_state'])\n",
    "\n",
    "    fold = 0\n",
    "    for train_ix, test_ix in cv_outer.split(dfx, dfy):\n",
    "    #for fold in range(5):\n",
    "        X_train = dfx.iloc[train_ix]\n",
    "        y_train = dfy.iloc[train_ix]\n",
    "\n",
    "        model_pipeline = results[model_name]['melhores_modelos'][fold]\n",
    "        model_predictor = model_pipeline.named_steps['predictor']\n",
    "        \n",
    "        model_pipeline.fit(X_train, y_train)\n",
    "        #model_pipeline.fit(dfx, dfy) # PENSAR: refazer o cv_outer ?\n",
    "\n",
    "        features = model_pipeline.named_steps['preproc'].get_feature_names_out()\n",
    "        importances = model_predictor.feature_importances_\n",
    "        plot_importance(importances, \n",
    "                        title=f\"Importância das features no modelo {model_name} / fold {fold+1}\", \n",
    "                        feature_names=features, max_features=20, \n",
    "                        ax=axs[fold])\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostras as importâncias médias dos modelos agrupadas pelos que usam features semelhantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for model_name in results.keys():\n",
    "    if 'vote' in model_name:\n",
    "        continue\n",
    "\n",
    "    model_pipeline = results[model_name]['melhores_modelos'][0]  # the best model found in the first fold\n",
    "    model_predictor = model_pipeline.named_steps['predictor']\n",
    "\n",
    "    # testa se o modelo guarda as importâncias das features\n",
    "    if not hasattr(model_predictor, 'feature_importances_'):\n",
    "        print(f\"O modelo {model_name} não registra as importâncias das features\")\n",
    "        continue\n",
    "\n",
    "    map_num_features_to_importances = dict()\n",
    "    \n",
    "    # groups the models that uses similarly modified features\n",
    "    for fold in range(5):\n",
    "        model_pipeline = results[model_name]['melhores_modelos'][fold]\n",
    "        model_predictor = model_pipeline.named_steps['predictor']\n",
    "        \n",
    "        # não precisa, porque já foi treinado na célula anterior\n",
    "        #model_pipeline.fit(...)\n",
    "\n",
    "        features = model_pipeline.named_steps['preproc'].get_feature_names_out()\n",
    "\n",
    "        num_features = len(features)\n",
    "        #print(\"Fold\", fold, \":\", num_features)\n",
    "\n",
    "        importances = model_predictor.feature_importances_\n",
    "        assert len(importances) == num_features\n",
    "\n",
    "        if num_features not in map_num_features_to_importances:\n",
    "            map_num_features_to_importances[num_features] = []\n",
    "        \n",
    "        map_num_features_to_importances[num_features].append((features, importances))\n",
    "\n",
    "    # plot the importances for each group of models\n",
    "    num_groups = len(map_num_features_to_importances)\n",
    "    fig, axs = plt.subplots(num_groups, 1, figsize=(14, 5*num_groups))\n",
    "    \n",
    "    for subplot, num_features in enumerate(map_num_features_to_importances.keys()):\n",
    "        list_of_importances = map_num_features_to_importances[num_features]\n",
    "        num_models = len(list_of_importances)\n",
    "        \n",
    "        # just to assure that the feature names are the same\n",
    "        # across the different model configurations\n",
    "        for j in range(len(list_of_importances)-1):\n",
    "            assert (list_of_importances[j][0] == list_of_importances[j+1][0]).all()\n",
    "\n",
    "        features = list_of_importances[0][0]\n",
    "        features = translate_feature_names(features)\n",
    "        mean_importances = np.mean([imp for (_, imp) in list_of_importances], axis=1)\n",
    "        \n",
    "        if num_groups > 1:\n",
    "            param_ax = axs[subplot]\n",
    "        else:\n",
    "            param_ax = axs\n",
    "        \n",
    "        # ATENÇÃO: isso se aplica aos modelos com dados apenas de Lula ou apenas de Bolsonaro (o geral deve ter 43 features)\n",
    "        cat_feature_encoding = 'one-hot encoding' if len(features)==42 else 'target encoding'\n",
    "        letter_s_if_plural   = 's' if num_models > 1 else ''\n",
    "\n",
    "        plot_importance(importances, \n",
    "                        title=f\"Random Forest's average Gini importance ({num_models} configuration{letter_s_if_plural} with {cat_feature_encoding})\", \n",
    "                        feature_names=features, \n",
    "                        max_features=20, \n",
    "                        ax=param_ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIM"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOuiv0RFkkwcXnb26LS0lwg",
   "provenance": [
    {
     "file_id": "1y8SEBwz0ltHkmfXNFpsBTWvHkCb-CFdt",
     "timestamp": 1676506165799
    }
   ]
  },
  "kernelspec": {
   "display_name": "socialnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
